Optimizer Zoo: Reimplementing PyTorch Optimizers from Scratch

A **lightweight, research-oriented project** that **reimplements popular PyTorch optimizers** entirely from scratch using only **basic tensor operations.** Ideal for understanding the core mechanics of optimization algorithms and benchmarking their performance.

## ðŸ“‹ Table of Contents
* [ðŸ“‚ Project Structure](#-project-structure)
* [ðŸš€ Implemented Optimizers](#-implemented-optimizers)
* [ðŸ§  Experiments](#-experiments)
* [âš™ï¸ Usage](#-usage)
* [ðŸ“ˆ Results](#-results)
* [ðŸ§© Dependencies](#-dependencies)
* [ðŸ“š Future Work](#-future-work)
* [ðŸ§¾ License](#-license)
* [âœ¨ Author](#-author)

---

## ðŸ“‚ Project Structure

optimizers/
â”œâ”€â”€ data/MNIST/raw/                    # MNIST dataset storage
â”œâ”€â”€ docs/                              # Documentation files
â”œâ”€â”€ experiments/                       # Experimental scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mnist_logreg.py               # Logistic regression experiment
â”‚   â”œâ”€â”€ mnist_mlp.py                  # MLP experiment
â”‚   â””â”€â”€ toy_function.py               # Quadratic function optimization test
â”œâ”€â”€ models/                           # Neural network architectures
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â””â”€â”€ mlp.py
â”œâ”€â”€ optimizers/                       # Custom optimizer implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adabelief.py
â”‚   â”œâ”€â”€ adagrad.py
â”‚   â”œâ”€â”€ adam.py
â”‚   â”œâ”€â”€ adamw.py
â”‚   â”œâ”€â”€ amsgrad.py
â”‚   â”œâ”€â”€ clr.py
â”‚   â”œâ”€â”€ lookahead.py
â”‚   â”œâ”€â”€ momentum.py
â”‚   â”œâ”€â”€ nadam.py
â”‚   â”œâ”€â”€ nesterov.py
â”‚   â”œâ”€â”€ radam.py
â”‚   â”œâ”€â”€ rmsprop.py
â”‚   â””â”€â”€ sgd.py
â”œâ”€â”€ results/                          # Output directories
â”‚   â”œâ”€â”€ checkpoints/                  # Model checkpoints
â”‚   â”œâ”€â”€ logs/                         # Training logs and JSON results
â”‚   â””â”€â”€ loss_curves/                  # Loss and accuracy plots
â”œâ”€â”€ utils/                            # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py                 # Data loading and preprocessing
â”‚   â”œâ”€â”€ loss_functions.py             # Loss function implementations
â”‚   â”œâ”€â”€ plot_utils.py                 # Visualization utilities
â”‚   â”œâ”€â”€ save_all_result.py            # Batch result saving
â”‚   â””â”€â”€ save_results.py               # Individual result saving
â”œâ”€â”€ run_all_experiments.py            # Main execution script
â””â”€â”€ README.md


---

## ðŸš€ Implemented Optimizers

| Optimizer | Key Features |
|------------|-----------|
| **SGD** | Vanilla gradient descent |
| **Momentum** | Velocity-based acceleration |
| **Nesterov** | Nesterov Accelerated Gradient (NAG) |
| **Adagrad** | Adaptive per-parameter learning rates |
| **RMSProp** | Exponentially decayed moving average of squared gradients |
| **Adam** | Momentum + RMSProp |
| **AdamW** | Decoupled weight decay |
| **AdaBelief** | Adaptive step based on belief in the gradient |
| **AMSGrad** | Adam with non-decreasing denominator |
| **Nadam** | Adam + Nesterov momentum |
| **RAdam** | Rectified Adam for variance correction |
| **CyclicLR (CLR)** | Learning rate scheduling with cycles |
| **Lookahead** | Slow/fast optimizer combination |

---

## ðŸ§  Experiments

Two baseline experiments are included:

| Script | Model | Dataset |Description |
|--------|--------|-------------| ---------|
| `toy_function.py` | - | Quadratic Function | Tests convergence on f(x) = xÂ² + 3x + 2 |
| `mnist_logreg.py` | Logistic Regression | MNIST | Linear classifier comparison |
| `mnist_mlp.py` | 2-layer MLP | MNIST | Nonlinear network comparison |

Experiment Metrics
*Training Loss: Convergence behavior over epochs

*Test Accuracy: Final performance on unseen data

*Training Time: Computational efficiency

*Convergence Speed: Iterations to reach target accuracy

---

## âš™ï¸ Usage

### 1. Setup Environment (Optional but Recommended)
```bash
# Install dependencies
pip install torch numpy matplotlib tqdm
```

### 2. Run Experiments
Run all experiments sequentially:

```bash
python run_all_experiments.py
```

Run individual experiments:
```bash
# Toy function optimization test
python -m experiments.toy_function

# Logistic regression on MNIST
python -m experiments.mnist_logreg

# MLP on MNIST  
python -m experiments.mnist_mlp
```

### 3.  Custom Usage

In each script:

```python
from optimizers import Adam
optimizer = Adam(model.parameters(), lr=1e-3)
```

---

ðŸ“ˆ Results
Results are automatically saved to:

*results/logs/ - JSON files with detailed metrics

*results/loss_curves/ - PNG plots of training curves

Sample Output Files:

*toy_function_results.json

*logreg_results.json

*mlp_results.json

*experiment_summary.json

Visualization Includes:

*Training loss vs. epochs

*Test accuracy vs. epochs

*Optimizer comparison plots

*Performance summary tables


---

## ðŸ§© Dependencies
The required packages can be installed using `pip install -r requirements.txt` (recommended).

| Package | Purpose | Version |
|---------|-----------------|------|
| `torch` | Tensor operations and autograd | >=2.0.0 |
| `numpy` | Numerical computations | >=1.21.0 |
| `matplotlib` | Result visualization | >=3.5.0 |
| `tqdm` | Progress bars (optional) | 	>=4.64.0 |

---

ðŸ“š Future Work

Implement additional optimizers (Lion, Sophia, Adan)

Add support for CIFAR-10 and custom datasets

Integrate TensorBoard for real-time visualization

Add distributed training support

Create interactive comparison dashboard

Add hyperparameter optimization scripts

---

ðŸ§¾ License

This project is licensed under the MIT License. Feel free to use, modify, and distribute for both academic and commercial purposes.

---

## âœ¨ Author

*Developed by **[Po Hung, Cheng]**
* GitHub: `[]`
A comprehensive study and reimplementation of optimization algorithms for deep learning.

---