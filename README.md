Optimizer Zoo: Reimplementing PyTorch Optimizers from Scratch

A **lightweight, research-oriented project** that **reimplements popular PyTorch optimizers** entirely from scratch using only **basic tensor operations.** Ideal for understanding the core mechanics of optimization algorithms and benchmarking their performance.

## ðŸ“‹ Table of Contents
* [ðŸ“‚ Project Structure](#-project-structure)
* [ðŸš€ Implemented Optimizers](#-implemented-optimizers)
* [ðŸ§  Experiments](#-experiments)
* [âš™ï¸ Usage](#-usage)
* [ðŸ“ˆ Results](#-results)
* [ðŸ§© Dependencies](#-dependencies)
* [ðŸ“š Future Work](#-future-work)
* [ðŸ§¾ License](#-license)
* [âœ¨ Author](#-author)

---

## ðŸ“‚ Project Structure

```text
optimizer-zoo/
â”œâ”€â”€ data/                  # MNIST dataset storage (ignored by git)
â”‚   â””â”€â”€ MNIST/raw/
â”œâ”€â”€ results/               # Training outputs (ignored by git)
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ loss_curves/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mnist_logreg.py
â”‚   â”œâ”€â”€ mnist_mlp.py
â”‚   â””â”€â”€ toy_function.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â””â”€â”€ mlp.py
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adabelief.py
â”‚   â”œâ”€â”€ adagrad.py
â”‚   â”œâ”€â”€ adam.py
â”‚   â”œâ”€â”€ adamw.py
â”‚   â”œâ”€â”€ amsgrad.py
â”‚   â”œâ”€â”€ clr.py
â”‚   â”œâ”€â”€ lookahead.py
â”‚   â”œâ”€â”€ momentum.py
â”‚   â”œâ”€â”€ nadam.py
â”‚   â”œâ”€â”€ nesterov.py
â”‚   â”œâ”€â”€ radam.py
â”‚   â”œâ”€â”€ rmsprop.py
â”‚   â””â”€â”€ sgd.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ loss_functions.py
â”‚   â”œâ”€â”€ plot_utils.py
â”‚   â”œâ”€â”€ save_all_result.py
â”‚   â””â”€â”€ save_results.py
â”œâ”€â”€ run_all_experiments.py
â””â”€â”€ README.md
```



---

## ðŸš€ Implemented Optimizers

| Optimizer | Key Features |
|------------|-----------|
| **SGD** | Vanilla gradient descent |
| **Momentum** | Velocity-based acceleration |
| **Nesterov** | Nesterov Accelerated Gradient (NAG) |
| **Adagrad** | Adaptive per-parameter learning rates |
| **RMSProp** | Exponentially decayed moving average of squared gradients |
| **Adam** | Momentum + RMSProp |
| **AdamW** | Decoupled weight decay |
| **AdaBelief** | Adaptive step based on belief in the gradient |
| **AMSGrad** | Adam with non-decreasing denominator |
| **Nadam** | Adam + Nesterov momentum |
| **RAdam** | Rectified Adam for variance correction |
| **CyclicLR (CLR)** | Learning rate scheduling with cycles |
| **Lookahead** | Slow/fast optimizer combination |

---

## ðŸ§  Experiments

Two baseline experiments are included:

| Script | Model | Dataset |Description |
|--------|--------|-------------| ---------|
| `toy_function.py` | - | Quadratic Function | Tests convergence on f(x) = xÂ² + 3x + 2 |
| `mnist_logreg.py` | Logistic Regression | MNIST | Linear classifier comparison |
| `mnist_mlp.py` | 2-layer MLP | MNIST | Nonlinear network comparison |

Experiment Metrics
Â·Training Loss: Convergence behavior over epochs

Â·Test Accuracy: Final performance on unseen data

Â·Training Time: Computational efficiency

Â·Convergence Speed: Iterations to reach target accuracy

---

## âš™ï¸ Usage

### 1. Setup Environment (Optional but Recommended)
```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Run Experiments
Run all experiments sequentially:

```bash
python run_all_experiments.py
```

Run individual experiments:
```bash
# Toy function optimization test
python -m experiments.toy_function

# Logistic regression on MNIST
python -m experiments.mnist_logreg

# MLP on MNIST  
python -m experiments.mnist_mlp
```

### 3.  Custom Usage

In each script:

```python
from optimizers import Adam
optimizer = Adam(model.parameters(), lr=1e-3)
```

---

ðŸ“ˆ Results
Results are automatically saved to:

Â·results/logs/ - JSON files with detailed metrics

Â·results/loss_curves/ - PNG plots of training curves

Visualization Includes:

Â·Training loss vs. epochs

Â·Test accuracy vs. epochs

Â·Optimizer comparison plots



---

## ðŸ§© Dependencies
The required packages can be installed using `pip install -r requirements.txt` (recommended).

| Package | Purpose | Version |
|---------|-----------------|------|
| `torch` | Tensor operations and autograd | >=2.0.0 |
| `numpy` | Numerical computations | >=1.21.0 |
| `matplotlib` | Result visualization | >=3.5.0 |
| `tqdm` | Progress bars (optional) | 	>=4.64.0 |

---

ðŸ“š Future Work

Implement additional optimizers (Lion, Sophia, Adan)

Add support for CIFAR-10 and custom datasets

Integrate TensorBoard for real-time visualization

Add distributed training support

Create interactive comparison dashboard

Add hyperparameter optimization scripts

---

ðŸ§¾ License

This project is licensed under the MIT License. Feel free to use, modify, and distribute for both academic and commercial purposes.

---

## âœ¨ Author

Â·Developed by **[Po Hung, Cheng]**
Â·GitHub: `[https://github.com/alexjeng0404/optimizer-zoo/blob/main/README.md]`
A comprehensive study and reimplementation of optimization algorithms for deep learning.

---